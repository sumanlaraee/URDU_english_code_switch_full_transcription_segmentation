# -*- coding: utf-8 -*-
"""segmentation and diarization using whisper and pyonnate gpu based script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Lr_JC5M_guNggxHhT3dFgxORO20mm4k
"""

# Whisper (from GitHub) and core libraries
!pip install git+https://github.com/openai/whisper.git
!pip install torch torchvision torchaudio pydub
!pip install pyannote.audio

# Ensure ffmpeg is installed (usually already available in Colab)
!apt-get install -y ffmpeg

# =========================
# üì¶ STEP 1: INSTALL DEPENDENCIES
# =========================

!pip install git+https://github.com/openai/whisper.git
!pip install pyannote-audio pydub torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!sudo apt-get install -y ffmpeg

# =========================
# ‚öôÔ∏è STEP 2: IMPORTS AND DEVICE SETUP
# =========================

import os
import torch
from pydub import AudioSegment
from pyannote.audio import Pipeline
import whisper
from pyannote.core import Segment

print("‚úÖ CUDA available:", torch.cuda.is_available())
print("Torch version:", torch.__version__)
print("CUDA version:", torch.version.cuda)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("üñ•Ô∏è Using device:", device)

# =========================
# üîÅ STEP 3: UPLOAD AUDIO FILE
# =========================

from google.colab import files
uploaded = files.upload()

# Replace with your actual file name if different
AUDIO_PATH = list(uploaded.keys())[0]  # Automatically uses uploaded file name
CHUNK_DURATION_MIN = 5

# =========================
# üîä STEP 4: SPLIT AUDIO INTO CHUNKS
# =========================

def split_audio(input_path, chunk_length_min=5):
    audio = AudioSegment.from_file(input_path)
    chunk_length_ms = chunk_length_min * 60 * 1000
    chunks = []
    os.makedirs("chunks", exist_ok=True)
    for i, start in enumerate(range(0, len(audio), chunk_length_ms)):
        end = min(start + chunk_length_ms, len(audio))
        chunk = audio[start:end]
        chunk_path = f"chunks/chunk_{i:03d}.wav"
        chunk.export(chunk_path, format="wav")
        chunks.append(chunk_path)
    return chunks

# =========================
# ü§ñ STEP 5: LOAD MODELS TO GPU
# =========================

HUGGINGFACE_TOKEN = "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"  # Replace with your own token

pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization@2.1",
    use_auth_token=HUGGINGFACE_TOKEN
).to(device)

model = whisper.load_model("large", device=device)

# =========================
# üß† STEP 6: DIARIZATION & TRANSCRIPTION
# =========================

chunks = split_audio(AUDIO_PATH, CHUNK_DURATION_MIN)
all_results = []

for chunk_path in chunks:
    print(f"\nüì¶ Processing {chunk_path} ...")
    diarization = pipeline(chunk_path)
    result = model.transcribe(chunk_path, language="ur", task="translate")

    for ws in result["segments"]:
        whisper_seg = Segment(ws["start"], ws["end"])
        for turn in diarization.itertracks(yield_label=True):
            spk_seg, _, speaker = turn
            if whisper_seg.intersects(spk_seg):
                aligned = {
                    "speaker": speaker,
                    "start": ws["start"],
                    "end": ws["end"],
                    "text": ws["text"]
                }
                all_results.append(aligned)
                break

# =========================
# üíæ STEP 7: SAVE OUTPUT
# =========================

with open("final_aligned_transcript.txt", "w", encoding="utf-8") as f:
    for entry in all_results:
        f.write(f"{entry['speaker']} [{entry['start']:.2f}-{entry['end']:.2f}]: {entry['text']}\n")

print("\n‚úÖ Done. Output saved to final_aligned_transcript.txt")

# =========================
# üì§ OPTIONAL: DOWNLOAD THE TRANSCRIPT
# =========================

files.download("final_aligned_transcript.txt")